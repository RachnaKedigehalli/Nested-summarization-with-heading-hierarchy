{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title Generation Experiment","metadata":{}},{"cell_type":"markdown","source":"For title generation, we have experimented on top of the following works: \n* https://arxiv.org/pdf/1512.01712.pdf\n* https://github.com/NainiShah/News-Headline-Generation\n\nThough we could not obtain satisfactory results due to training for less iterations (memory constraint).","metadata":{}},{"cell_type":"code","source":"import random\nimport codecs\nimport math\nimport time\nimport sys\nimport subprocess\nimport os.path\nimport pickle\nimport numpy as np\nimport gensim\n\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Lambda, Activation\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\n\nfrom tqdm import tqdm\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom numpy import inf\nfrom operator import itemgetter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ftfy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport os\nfrom collections import defaultdict\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob, Word\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import KeyedVectors\nimport ftfy\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 28\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_freq_word_to_use = 40000\nembedding_dimension = 300\nmax_len_head = 25\nmax_len_desc = 50\nmax_length = max_len_head + max_len_desc\nrnn_layers = 4\nrnn_size = 128\nactivation_rnn_size = 16\nempty_tag_location = 0\neos_tag_location = 1\nunknown_tag_location = 2\nlearning_rate = 1e-4\nmin_head_line_gen = 10\ndont_repeat_word_in_last = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec = []\nidx2word = {}\nword2idx = {}\n\nword2idx['<empty>'] = empty_tag_location\nword2idx['<eos>'] = eos_tag_location\nword2idx['<unk>'] = unknown_tag_location\nidx2word[empty_tag_location] = '<empty>'\nidx2word[eos_tag_location] = '<eos>'\nidx2word[unknown_tag_location] = '<unk>'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_word_embedding(file_name):\n    idx = 3\n    temp_word2vec_dict = {}\n    \n    temp_word2vec_dict['<empty>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n    temp_word2vec_dict['<eos>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n    temp_word2vec_dict['<unk>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n    model = gensim.models.KeyedVectors.load_word2vec_format(file_name, binary = True, limit = 40000)\n    V = model.index_to_key\n    X = np.zeros((top_freq_word_to_use, model.vector_size))\n    for index, word in tqdm(enumerate(V)):\n        vector = model[word]\n        temp_word2vec_dict[idx] = vector\n        word2idx[word] = idx\n        idx2word[idx] = word\n        idx = idx + 1\n            \n    return temp_word2vec_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_word2vec_dict = read_word_embedding('../input/googlenewsvectors/GoogleNews-vectors-negative300.bin')\nlength_vocab = len(temp_word2vec_dict)\nshape = (length_vocab, embedding_dimension)\n\nword2vec = np.random.uniform(low=-1, high=1, size=shape)\nfor i in range(length_vocab):\n    if i in temp_word2vec_dict:\n        word2vec[i, :] = temp_word2vec_dict[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simple_context(X, mask):\n    \n    desc, head = X[:, :max_len_desc, :], X[:, max_len_desc:, :]\n\n    head_activations, head_words = head[:, :, :activation_rnn_size], head[:, :, activation_rnn_size:]\n    desc_activations, desc_words = desc[:, :, :activation_rnn_size], desc[:, :, activation_rnn_size:]\n\n    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2, 2))\n\n\n    activation_energies = activation_energies + -1e20 * K.expand_dims(1. - K.cast(mask[:, :max_len_desc], 'float32'), 1)\n\n\n    activation_energies = K.reshape(activation_energies, (-1, max_len_desc))\n    activation_weights = K.softmax(activation_energies)\n    activation_weights = K.reshape(activation_weights, (-1, max_len_head, max_len_desc))\n\n    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2, 1))\n    return K.concatenate((desc_avg_word, head_words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def output_shape_simple_context_layer(input_shape):\n    return (input_shape[0], max_len_head , 2 * (rnn_size - activation_rnn_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n\n        model = Sequential()\n\n        model.add(\n                Embedding(\n                        40003, embedding_dimension,\n                        input_length=max_length,\n                        mask_zero=True,\n                        name='embedding_layer'\n                )\n        )\n\n        for i in range(rnn_layers):\n            lstm = LSTM(rnn_size, return_sequences=True,\n                name='lstm_layer_%d' % (i + 1)\n            )\n\n            model.add(lstm)\n\n        model.add(Lambda(simple_context,\n                     mask=lambda inputs, mask: mask[:, max_len_desc:],\n                     output_shape=output_shape_simple_context_layer,\n                     name='simple_context_layer'))\n\n        vocab_size = 40003\n        model.add(TimeDistributed(Dense(vocab_size,\n                                name='time_distributed_layer')))\n        \n        model.add(Activation('softmax', name='activation_layer'))\n        \n        model.compile(loss='categorical_crossentropy', optimizer='adam')\n        K.set_value(model.optimizer.lr, np.float32(learning_rate))\n        print (model.summary())\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tagging","metadata":{}},{"cell_type":"code","source":"def padding(list_idx, curr_max_length, is_left):\n    if len(list_idx) >= curr_max_length:\n        return list_idx\n    number_of_empty_fill = curr_max_length - len(list_idx)\n    if is_left:\n        return [empty_tag_location, ] * number_of_empty_fill + list_idx\n    else:\n        return list_idx + [empty_tag_location, ] * number_of_empty_fill","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def headline2idx(list_idx, curr_max_length, is_input):\n    if is_input:\n        if len(list_idx) >= curr_max_length - 1:\n            return list_idx[:curr_max_length - 1]\n        else:\n            list_idx = list_idx + [eos_tag_location, ]\n            return padding(list_idx, curr_max_length - 1, False)\n    else:\n        if len(list_idx) == curr_max_length:\n            list_idx[-1] = eos_tag_location\n            return list_idx\n        else:\n            list_idx = list_idx + [eos_tag_location, ]\n            return padding(list_idx, curr_max_length, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def desc2idx(list_idx, curr_max_length):\n    list_idx.reverse()\n    list_idx = padding(list_idx, curr_max_length, True)\n    list_idx = list_idx + [eos_tag_location, ]\n    return list_idx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence2idx(sentence, is_headline, curr_max_length, is_input=True):\n    list_idx = []\n    tokens = sentence.split(\" \")\n    count = 0\n    for each_token in tokens:\n        if each_token in word2idx:\n            list_idx.append(word2idx[each_token])\n        else:\n            list_idx.append(word2idx['<unk>'])\n        count = count + 1\n        if count >= curr_max_length:\n            break\n\n    if is_headline:\n        return headline2idx(list_idx, curr_max_length, is_input)\n    else:\n        return desc2idx(list_idx, curr_max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flip_words_randomly(description_headline_data, number_words_to_replace, model):\n    if number_words_to_replace <= 0 or model == None:\n        return description_headline_data\n\n    assert np.all(description_headline_data[:, max_len_desc] == eos_tag_location)\n\n    batch_size = len(description_headline_data)\n    predicted_headline_word_idx = model.predict(description_headline_data, verbose=1, batch_size = batch_size)\n    copy_data = description_headline_data.copy()\n    for idx in range(batch_size):\n        random_flip_pos = sorted(random.sample(range(max_len_desc + 1, max_length), number_words_to_replace))\n        for replace_idx in random_flip_pos:\n            if (description_headline_data[idx, replace_idx] == empty_tag_location or\n            description_headline_data[idx, replace_idx] == eos_tag_location):\n                continue\n\n            new_id = replace_idx - (max_len_desc + 1)\n            prob_words = predicted_headline_word_idx[idx, new_id]\n            word_idx = prob_words.argmax()\n\n            if word_idx == empty_tag_location or word_idx == eos_tag_location:\n                continue\n            copy_data[idx, replace_idx] = word_idx\n    return copy_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_inputs(descriptions, headlines, number_words_to_replace, model, is_training):\n    assert len(descriptions) == len(headlines)\n\n    X, y = [], []\n    for each_desc, each_headline in zip(descriptions, headlines):\n        input_headline_idx = sentence2idx(each_headline, True, max_len_head, True)\n        predicted_headline_idx = sentence2idx(each_headline, True, max_len_head, False)\n        desc_idx = sentence2idx(each_desc, False, max_len_desc)\n        \n        assert len(input_headline_idx) == max_len_head - 1\n        assert len(predicted_headline_idx) == max_len_head\n        assert len(desc_idx) == max_len_desc + 1\n\n        X.append(desc_idx + input_headline_idx)\n        y.append(predicted_headline_idx)\n        \n    X, y = np.array(X), np.array(y)\n    if is_training:\n\n        X = flip_words_randomly(X, number_words_to_replace, model)\n\n        vocab_size = word2vec.shape[0]\n        length_of_data = len(headlines)\n        Y = np.zeros((length_of_data, max_len_head, vocab_size))\n        for i, each_y in enumerate(y):\n            Y[i, :, :] = np_utils.to_categorical(each_y, vocab_size)\n        assert len(X)==len(Y)\n        return X, Y\n    else:\n        return X,headlines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def shuffle_file(file_name):\n    try:\n        subprocess.check_output(['shuf',file_name,\"--output=\"+file_name])\n        print (\"Input file shuffled\")\n    except:\n        print (\"shuf command not available!\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def large_file_reading_generator(data):\n    while True:\n        for each_line in data.items():\n            yield each_line","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(file_name,number_words_to_replace,model,is_training=True):\n    with open(file_name,'rb') as file_pointer:\n        data = pickle.load(file_pointer)\n        headlines_data = data['heads']\n        descs_data = data['descs']\n    headline_iterator = large_file_reading_generator(headlines_data)\n    descs_iterator = large_file_reading_generator(descs_data)\n    while True:\n        X, y = [], []\n        for i in range(128):\n            heads_line = next(headline_iterator)\n            descs_line = next(descs_iterator)\n            heads_line = heads_line[1]\n            descs_line = descs_line[1]\n            X.append(descs_line)\n            y.append(heads_line)\n        yield convert_inputs(X, y, number_words_to_replace, model,is_training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def OHE_to_indexes(y_val):\n    list_of_headline = []\n    for each_headline in y_val:\n        list_of_word_indexes = np.where(np.array(each_headline)==1)[1]\n        list_of_headline.append(list(list_of_word_indexes))\n    return list_of_headline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def indexes_to_words(list_of_headline):\n    list_of_word_headline = []\n    for each_headline in list_of_headline:\n        each_headline_words = []\n        for each_word in each_headline:\n            if each_word in (empty_tag_location, eos_tag_location, unknown_tag_location):\n                continue\n            each_headline_words.append(idx2word[each_word])\n        list_of_word_headline.append(each_headline_words)            \n    return list_of_word_headline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def blue_score_text(y_actual, y_predicted):\n    assert len(y_actual) ==  len(y_predicted)\n    no_of_news = len(y_actual)\n    blue_score = 0.0\n    for i in range(no_of_news):\n        reference = y_actual[i]\n        hypothesis = y_predicted[i]\n        \n        weights=(0.25, 0.25, 0.25, 0.25)\n        min_len_present = min(len(reference),len(hypothesis))\n        if min_len_present==0:\n            continue\n        if min_len_present<4:\n            weights=[1.0/min_len_present,]*min_len_present\n            \n        blue_score = blue_score + sentence_bleu([reference],hypothesis,weights=weights)\n        \n    return blue_score/float(no_of_news)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def blue_score_calculator(model, validation_file_name, no_of_validation_sample, validation_step_size):\n    number_words_to_replace=0\n    temp_gen = data_generator(validation_file_name,number_words_to_replace, model)        \n        \n    total_blue_score = 0.0            \n    blue_batches = 0\n    blue_number_of_batches = no_of_validation_sample / validation_step_size\n    for X_val, y_val in temp_gen:\n        predict_x = model.predict(X_val, batch_size = validation_step_size,verbose = 1) \n        y_predicted = np.argmax(predict_x,axis=1)\n\n        y_predicted_words = indexes_to_words(y_predicted)\n        list_of_word_headline = indexes_to_words(OHE_to_indexes(y_val))\n        assert len(y_val)==len(list_of_word_headline) \n\n        total_blue_score = total_blue_score + blue_score_text(list_of_word_headline, y_predicted_words)\n            \n        blue_batches += 1\n        if blue_batches >=  blue_number_of_batches:\n            break\n        if blue_batches%10==0:\n            print (\"eval for {} out of {}\".format(blue_batches, blue_number_of_batches))\n\n    del temp_gen\n    return total_blue_score/float(blue_batches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model,data_file,val_file,train_size,val_size,val_step_size,epochs,words_replace_count,model_weights_file_name):\n    if os.path.isfile(model_weights_file_name):\n        print (\"loading weights already present in {}\".format(model_weights_file_name))\n        model.load_weights(model_weights_file_name)\n        print (\"model weights loaded for further training\")\n            \n    train_data = data_generator(data_file,words_replace_count, model)\n    blue_scores = []\n    best_blue_score_track = -1.0\n    number_of_batches = math.ceil(train_size / float(128))\n        \n    for each_epoch in range(epochs):\n        print (\"running for epoch \",each_epoch)\n        start_time = time.time()\n        batches = 0\n        for X_batch, Y_batch in train_data:\n            model.fit(X_batch,Y_batch,batch_size=128,epochs=1)\n            batches += 1\n            if batches >= number_of_batches :\n                break\n            if batches%10==0:\n                print (\"training for {} out of {} for epoch {}\".format(batches, number_of_batches, each_epoch))\n                    \n        end_time = time.time()\n        print(\"time to train epoch \",end_time-start_time)\n\n        blue_score_now = blue_score_calculator(model,val_file,val_size,val_step_size)\n        blue_scores.append(blue_score_now)\n        if best_blue_score_track < blue_score_now:\n            best_blue_score_track = blue_score_now\n            print (\"saving model for blue score \",best_blue_score_track)\n            model.save_weights(model_weights_file_name)\n                \n        with open(\"blue_scores.pickle\", \"wb\") as output_file:\n            pickle.dump(blue_scores, output_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_headline_end(word_index_list,current_prediction_position):\n    if (word_index_list is None) or (len(word_index_list)==0):\n        return False\n    if word_index_list[current_prediction_position]==eos_tag_location or current_prediction_position>=max_length:\n        return True\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_word(prediction,word_position_index,top_k,X,prev_layer_log_prob):\n    prediction = prediction[0]\n    prediction_at_word_index = prediction[word_position_index]\n    sorted_arg = prediction_at_word_index.argsort()\n    top_probable_indexes = sorted_arg[::-1]\n    top_probabilities = np.take(prediction_at_word_index,top_probable_indexes)\n    log_probabilities = np.log(top_probabilities)\n    log_probabilities[log_probabilities == -inf] = -sys.maxsize - 1\n    log_probabilities = log_probabilities + prev_layer_log_prob\n    assert len(log_probabilities)==len(top_probable_indexes)\n    \n    offset = max_len_desc+word_position_index+1\n    ans = []\n    count = 0 \n    for i,j in zip(log_probabilities, top_probable_indexes):\n        if j in X[max_len_desc+1:offset][-dont_repeat_word_in_last:]:\n            continue\n        if (word_position_index < min_head_line_gen) and (j in [empty_tag_location, unknown_tag_location, eos_tag_location]):\n            continue\n            \n        next_input = np.concatenate((X[:offset], [j,]))\n        next_input = next_input.reshape((1,next_input.shape[0]))\n        \n        if offset!=max_length:\n            next_input = sequence.pad_sequences(next_input, maxlen=max_length, value=empty_tag_location, padding='post', truncating='post')\n        next_input = next_input[0]\n        ans.append((i,next_input))\n        count = count + 1\n        if count>=top_k:\n            break\n    return ans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beam_search(model,X,top_k):\n    prev_word_index_top_k = []\n    curr_word_index_top_k = []\n    done_with_pred = []\n    data = X.reshape((1,X.shape[0]))\n    prediction = model.predict(data,verbose=0)\n    \n    prev_word_index_top_k = process_word(prediction,0,top_k,X,0.0)\n        \n    for i in range(1,max_len_head):\n        for j in range(len(prev_word_index_top_k)):\n            probability_now, current_intput = prev_word_index_top_k[j]\n            data = current_intput.reshape((1,current_intput.shape[0]))\n            prediction = model.predict(data,verbose=0)\n            next_top_k_for_curr_word = process_word(prediction,i,top_k,current_intput,probability_now)\n            curr_word_index_top_k = curr_word_index_top_k + next_top_k_for_curr_word\n                \n        curr_word_index_top_k = sorted(curr_word_index_top_k,key=itemgetter(0),reverse=True)\n        prev_word_index_top_k_temp = curr_word_index_top_k[:top_k]\n        curr_word_index_top_k = []\n        prev_word_index_top_k = []\n        for each_proba, each_word_idx_list in prev_word_index_top_k_temp:\n            offset = max_len_desc+i+1\n            if is_headline_end(each_word_idx_list,offset):\n                done_with_pred.append((each_proba, each_word_idx_list))\n            else:\n                prev_word_index_top_k.append((each_proba,each_word_idx_list))\n            \n    done_with_pred = sorted(done_with_pred,key=itemgetter(0),reverse=True)\n    done_with_pred = done_with_pred[:top_k]\n    return done_with_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, data_file_name, no_of_testing_sample, model_weights_file_name,top_k,output_file,seperator='#|#'):\n    model.load_weights(model_weights_file_name)\n    print (\"model weights loaded\")\n    test_batch_size = 1\n    test_data_generator = data_generator(data_file_name, number_words_to_replace=0, model=None,is_training=False)\n    number_of_batches = math.ceil(no_of_testing_sample / float(test_batch_size))\n        \n    with codecs.open(output_file, 'w',encoding='utf8') as f:\n        batches = 0\n        for X_batch, Y_batch in tqdm(test_data_generator):\n            X = X_batch[0]\n            Y = Y_batch[0]\n            assert X[max_len_desc]==eos_tag_location\n            X[max_len_desc+1:]=empty_tag_location\n            result = beam_search(model,X,top_k)\n            list_of_word_indexes = result[0][1]\n            list_of_words = indexes_to_words([list_of_word_indexes])[0]\n            headline = u\" \".join(list_of_words[max_len_desc+1:])\n            f.write(Y+seperator+headline+\"\\n\")\n            batches += 1\n            \n            if batches >= number_of_batches :\n                break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"stop = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(document):\n    return word_tokenize(document)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lower_case(tokens):\n    return [word.lower() for word in tokens]\ndef remove_punctuation(tokens):\n    return [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\ndef remove_stopwords(tokens):\n    return [word for word in tokens if word not in stop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_url(data):\n    return [re.sub(r'https://','', sentence) for sentence in data]\ndef remove_html(data):\n    return [BeautifulSoup(sentence, 'html.parser').get_text() for sentence in data]\ndef remove_bracket(data):\n    return [re.sub(r'[\\([{})\\]]','', sentence) for sentence in data]\ndef remove_digit(data):\n    return [re.sub('[0-9]','', sentence) for sentence in data]\ndef remove_underscore(data):\n    return [sentence.replace(\"_\",\"\") for sentence in data]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(text):\n    text = remove_url(text)\n    text = remove_html(text)\n    text = remove_bracket(text)\n    text = remove_digit(text)\n    text = remove_underscore(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data_file(df, file_name):\n    df = df[df.notnull()]\n    df.drop('url', axis=1)\n    df = df.dropna(how='any')\n    heads = df['title']\n    descs = df['content']\n    print(\"preprocessed\")\n    title_list = []\n    content_list = []\n    for i in heads:\n        try:\n            title = ftfy.fix_text(i)\n            title_list.append(i)\n        except:\n            print(i)\n            return\n    for i in descs:\n        try:\n            title = ftfy.fix_text(i)\n            content_list.append(i)\n        except:\n            print(i)\n            return\n    print(\"Beginning tokenization\")\n    tokenized_title = [tokenize(title) for title in title_list]\n    tokenized_content = [tokenize(content) for content in content_list]\n    tokenized_title = [remove_punctuation(title) for title in tokenized_title]\n    tokenized_content = [remove_punctuation(content) for content in tokenized_content]\n    print(\"Done tokenization\")\n    print(\"Beginning filtering\")\n    filtered_title = [remove_stopwords(lower_case(title)) for title in tokenized_title]\n    filtered_content = [remove_stopwords(lower_case(content)) for content in tokenized_content]\n    print(\"Done filtering\")\n    title_new = [' '.join(c for c in s if c not in string.punctuation) for s in filtered_title]\n    content_new = [' '.join(c for c in s if c not in string.punctuation) for s in filtered_content]\n    print(\"Cleared Punc\")\n    final_df = pd.DataFrame(\n    {'heads': title_new,\n     'descs': content_new,\n    })\n    print(\"Writing to file\")\n    final_df.to_pickle(file_name)\n    return final_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_to_file():\n    df = pd.read_pickle('../input/preproc-articles/article1.pickle')\n    print(\"created final df\")\n    train = df.iloc[:8000]\n    validation = df.iloc[8000:10000]\n    test = df.iloc[10000:12000]\n    print(\"writing to files\")\n    train.to_pickle('train_data.pkl')\n    validation.to_pickle('validation_data.pkl')\n    test.to_pickle('test_data.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(file_path, output_name):\n    df= pd.read_csv(file_path)\n    final_df = create_data_file(df, output_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load_data('../input/all-the-news/articles1.csv', 'article1.pickle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_to_file()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/all-the-news/articles2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['content', 'title']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train(model=model, \n#     data_file='./train_data.pkl', \n#     val_file='./validation_data.pkl',\n#     train_size=8000, \n#     val_size=2000,\n#     val_step_size=128,\n#     epochs=5,\n#     words_replace_count=5,\n#     model_weights_file_name='model_weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('../input/preproc-articles/model_weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test(model=test_model,\n#     data_file_name='test_data.pkl',\n#     no_of_testing_sample= 2000,\n#     model_weights_file_name='model_weights.h5',\n#     top_k=5,\n#     output_file='test_output.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_convert_inputs(descriptions, headlines):\n\n    X, y = [], []\n    input_headline_idx = sentence2idx(headlines, True, max_len_head, True)\n    predicted_headline_idx = sentence2idx(headlines, True, max_len_head, False)\n    desc_idx = sentence2idx(descriptions, False, max_len_desc)\n    assert len(input_headline_idx) == max_len_head - 1\n    assert len(predicted_headline_idx) == max_len_head\n    assert len(desc_idx) == max_len_desc + 1\n\n    X.append(desc_idx + input_headline_idx)\n    y.append(predicted_headline_idx)\n        \n    X, y = np.array(X), np.array(y)\n    print(\"Convert descs and headlines to numpy arrays\")\n    return X,headlines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_process_word(prediction,word_position_index,top_k,X,prev_layer_log_prob):\n    prediction = prediction[0]\n    prediction_at_word_index = prediction[word_position_index]\n    sorted_arg = prediction_at_word_index.argsort()\n    top_probable_indexes = sorted_arg[::-1]\n    top_probabilities = np.take(prediction_at_word_index,top_probable_indexes)\n    log_probabilities = np.log(top_probabilities)\n    log_probabilities[log_probabilities == -inf] = -sys.maxsize - 1\n    log_probabilities = log_probabilities + prev_layer_log_prob\n    assert len(log_probabilities)==len(top_probable_indexes)\n    \n    offset = max_len_desc+word_position_index+1\n    ans = []\n    count = 0 \n    for i,j in zip(log_probabilities, top_probable_indexes):\n        if j in X[max_len_desc+1:offset][-dont_repeat_word_in_last:]:\n            continue\n        if (word_position_index < min_head_line_gen) and (j in [empty_tag_location, unknown_tag_location, eos_tag_location]):\n            continue\n            \n        next_input = np.concatenate((X[:offset], [j,]))\n        next_input = next_input.reshape((1,next_input.shape[0]))\n        \n        if offset!=max_length:\n            next_input = sequence.pad_sequences(next_input, maxlen=max_length, value=empty_tag_location, padding='post', truncating='post')\n        next_input = next_input[0]\n        ans.append((i,next_input))\n        count = count + 1\n        if count>=top_k:\n            break\n    return ans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_beam_search(X,top_k):\n    prev_word_index_top_k = []\n    curr_word_index_top_k = []\n    done_with_pred = []\n    data = X.reshape((1,X.shape[0]))\n    \n    prediction = model.predict(data,verbose=0)\n    prev_word_index_top_k = predict_process_word(prediction,0,top_k,X,0.0)\n        \n    for i in range(1,max_len_head):\n        for j in range(len(prev_word_index_top_k)):\n            probability_now, current_intput = prev_word_index_top_k[j]\n            data = current_intput.reshape((1,current_intput.shape[0]))\n            prediction = model.predict(data,verbose=0)\n            next_top_k_for_curr_word = predict_process_word(prediction,i,top_k,current_intput,probability_now)\n            curr_word_index_top_k = curr_word_index_top_k + next_top_k_for_curr_word\n                \n        curr_word_index_top_k = sorted(curr_word_index_top_k,key=itemgetter(0),reverse=True)\n        prev_word_index_top_k_temp = curr_word_index_top_k[:top_k]\n        curr_word_index_top_k = []\n        prev_word_index_top_k = []\n        for each_proba, each_word_idx_list in prev_word_index_top_k_temp:\n            offset = max_len_desc+i+1\n            if is_headline_end(each_word_idx_list,offset):\n                done_with_pred.append((each_proba, each_word_idx_list))\n            else:\n                prev_word_index_top_k.append((each_proba,each_word_idx_list))\n            \n    done_with_pred = sorted(done_with_pred,key=itemgetter(0),reverse=True)\n    done_with_pred = done_with_pred[:top_k]\n    return done_with_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_title(descriptions,headlines,top_k,seperator='#|#'):\n    X,Y = predict_convert_inputs(descriptions, headlines)\n    X = X[0]\n    Y = Y[0]\n    assert X[max_len_desc]==eos_tag_location\n    X[max_len_desc+1:]=empty_tag_location\n    result = predict_beam_search(X,top_k)\n    list_of_word_indexes = []\n    list_of_word_indexes = result[0][1]\n    list_of_words = indexes_to_words([list_of_word_indexes])[0]\n    list_of_words.reverse()\n    list_of_words = list_of_words[10:]\n    print(list_of_words)\n    headline = \" \".join(list_of_words)\n    return str(headline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_title(descriptions=df['content'][7],\n              headlines=df['title'][7],\n    top_k=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['content'][7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}